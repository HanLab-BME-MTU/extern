%%								cgist.m
%		This is a general implementation of the CGIST algorithm that solves
%		the following L1 regularized problems:
%			(1) min   mu|x| + 1/2 || Ax - f ||^2  
%			
%			(2) min   mu|x| + 1/2 || Ax - f ||^2   such that x>0
%	
%			(3) min		|x|              such that || Ax - f || < mu
%			
%			(4) min   || Ax - f ||^2     such that |x|<mu
%
%			(5) min		|x|              such that || Ax - f || < mu, x>0
%			
%			(6) min   || Ax - f ||^2     such that |x|<mu, x>0
%
%
% Syntax:  [sol, multCount, subgradientNorm, out] 
%									= cgist(A,At,f,mu,regularizer {,opts})
%
% Inputs:
%	A  - Either an m by n matrix or a function handle representing the linear
%		  operator "A" above
%
%	At - If "A" is a function handle, then this is the handle to the adjoint
%			of "A". If "A" is a matrix, then simply use "At = []"
%   
%	f  - A dense column vector of length m
%
%	mu - The regularization parameter for the problem we want to solve.  
%	 
%	regularizer - A string specifying which of the 6 problems we wish to
%		 solve. The options are 'l1', 'nonnegative', 'l2_constrained',
%		 'l2_constrained_nonnegative', 'l1_constrained', and
%		 'l1_constrained_nonnegative' for problems 1-6, respectively.
%
%	opts - An optional struct containing fields which give the user more
%		 control.  The user may choose to set the following fields of
%		 "opts":
%			* opts.guess - a vector of length n containing the initial
%				guess.  Default value is a vector of zeros.
%			* opts.continuation_if_needed - if "true", then continuation
%				will be used to solve problems with high dynamic range.
%				Default value is "true".
%			* opts.tol - Iteration stops when the norm of the subgradient 
%				of the objective drops below this threshold.  Default is 1e-4.
%			* opts.max_iter - Maximum iterations per continuation step.
%				Default value is 2.5e4.
%			* opts.record_iterates - If "true" then the approximated solution
%				vector at each iteration is stored sequentially in the
%				cell array "out.iterates".  Default value is "false".
%			* opts.record_objective - If "true" save the value of the
%				objective function of each iterate to "out.objective".
%				Default value is "false".
%			* opts.record_subgradient_norms - If "true", save the norm of 
%				the subgradient vecotor at each iterate to
%				"out.subgradient_norms".  Default value is "false"
%			* opts.record_mults_per_continuation_step - If "true" record
%				the number of matrix multiplications for each continuation 
%				step to the vector "out.mults_per_continuation_step".
%				Default value is "false".
%   
%
% Outputs:
%    sol - The solution to the chosen optimization problem, a column
%			vecotor of length n.
%    multCount - The total number of matrix multiplications, counting both
%			"A" and it's adjoint separately, needed to solve the problem.
%    subgradientNorm - The norm of a vector in the subgradient of the
%			energy.  This serves as a certificate that the minimizer is
%			correct because the subgradient is nearly zero.
%	out -	A structure containing additional outputs.  Some of these
%			outputs are only recorded if the relavent flag is set in "opts".
%			The members of "out" are:
%			* out.num_continuation_steps - the number of times that a
%				subproblem of type (1) needed to be solved.
%			* out.iterates - A cell array containing each iterate x^k 
%				at every step of the method.  This array will be empty unless
%			*	you set the flag opts.record_iterates to "true".
%			* out.subgradient_norms -  A cell array containing the norm of
%				the subgradient at every step of the method.  This array 
%				will be empty unless you set the flag 
%				"opts.record_subgradient_norms" to "true".
%			* out.mults_per_continuation_step - An array containing the
%				number of matrix multiplications used by each continuation
%				sub-step.
%			* out.iterates_per_continuation_step - The number of iterates
%				generated by each continuation step.  This information can
%				be used to group the recorded iterates by continuation
%				step.
%
% Example: 
%    To solve a basis pursuit denoising problem of type 1 with a dense 
%	matrix "A", a vector "f", and scalar "mu", use the command
%			solution = cgist(A,[],f,mu,'l1');
%	For more extensive examples, see the file cgist_demo.m, which is
%	included in this distribution.
%
% Other m-files required: none
%
% See also: cgist_demo.m

% Authors: 
%		Thomas Goldstein
%		Department of Electrical Engineering
%		Stanford University
%		http://www.stanford.edu/~tagoldst
%
%		Simon Setzer
%		Department of Mathematics and Computer Science
%		Saarland University
%		http://www.mia.uni-saarland.de/setzer/index.shtml
%
%		Last Revised, 12 Feb 2011



function  [sol, multCount, subgradientNorm, out] = cgist(A,At,f,mu,regularizer,opts)

	%% Initialize: 

	%  If user did not hand in a function handle for A and At, then we are using
	%  an explicit matrix.  In this case, we build a function handle for it.
	if  ~isa(A,'function_handle')
		 At = A';
		 A = @(x) A*x;
		 At = @(x)At*x;
	end

	%   Compute A*f, and use it to figure out the size of the unknown signal
	Atf = At(f); 
	N = size(Atf,1);
	if ~exist('opts','var')
		opts = [];
	end
	
	%  Test that A and At are indeed Adjoints
	testVector = randn(N,1);
	inner1 = testVector'*Atf;
	inner2 = A(testVector)'*f;
	if(abs( inner1 - inner2 ) > abs(inner1)*1e-10 )
		error(['The adjoint of A is not At, <Ax,y> = ' num2str(inner1) ', <x,At*y>  =' num2str(inner2)]);
	end
	
	%  Fill in the fields of opts that the user didn't
	opts = completeOpts(N,opts);
	opts.mu_star = max(abs(Atf));	% this is the critical value of mu for which the solution is zero

	if ~isreal(mu) || mu<0
		error(['Invalid value for regularization parameter: mu = ' num2str(mu)]);
	end
	if strcmp(regularizer,'l1') || strcmp(regularizer,'nonnegative')
		[sol, multCount, subgradientNorm, out] = cgist_unconstrained(A,At,f,mu,regularizer,opts);
	elseif strcmp(regularizer,'l2_constrained')
		[sol, multCount, subgradientNorm, out] = cgist_l2_constrained(A,At,f,mu,'l1',opts);
	elseif strcmp(regularizer,'l2_constrained_nonnegative')
		[sol, multCount, subgradientNorm, out] = cgist_l2_constrained(A,At,f,mu,'nonnegative',opts);
	elseif strcmp(regularizer,'l1_constrained')
		[sol, multCount, subgradientNorm, out] = cgist_l1_constrained(A,At,f,mu,'l1',opts);
	elseif strcmp(regularizer,'l1_constrained_nonnegative')
		[sol, multCount, subgradientNorm, out] = cgist_l1_constrained(A,At,f,mu,'nonnegative',opts);		
	else
		error(['Invalid regularizer: ' regularizer]);
	end

return

%  This method solves l1 regularized least squares problems with either no
%  constraints, or non-negativeity constraints
function [sol, multCount, subgradientNorm, out] = cgist_unconstrained(A,At,f,mu,regularizer,opts)
	%% Figure out if we need to do continuation
	mu_new  = opts.mu_star/5;		
	multCount = 1;
	if opts.continuation_if_needed
		while mu_new > 5*mu    % Do continuation until mu_new is close to mu
			%disp('Continuation')
			[opts.guess, count, subgradientNorm, opts.out  ] = cgist_kernel(A,At,f,mu_new,regularizer,opts);
			multCount = multCount+count;
                %  Use Wright and Nowak's adaptive continuation
            Aty = At(f-A(opts.guess))/5; 
	        mu_new = max(abs(Aty));
		end
	end

	%% Solve the final problem
	[sol, multCount, subgradientNorm, out] = cgist_kernel(A,At,f,mu,regularizer,opts);
return


function [sol, multCount, subgradientNorm, out] = cgist_l2_constrained(A,At,f,eps,regularizer,opts)

		% set the tolerance in the norm constraint
	tol_norm = opts.tol*10;
	opts.tol = opts.tol/10;	%  make kernel solution more precise so we satisfy the norm tolerance
	norm_new = norm(A(opts.guess)-f);
	mu_new = opts.mu_star;
	multCount  = 2;
	if norm_new < eps+tol_norm   %	If the zero vector is a solution, then return
		sol = opts.guess;
		subgradientNorm = 0;
		out = opts.out;
		return
	end

	while  norm_new > eps     % Do continuation until mu_new is small enough to satisfy the L2 constraint
		mu_old = mu_new;
		norm_old = norm_new;
		u_old = opts.guess;

		     %  Use Wright and Nowak's adaptive continuation
         Aty = At(f-A(u_old))/5; 
	     mu_new = max(abs(Aty));
		
         [opts.guess, count, subgradientNorm opts.out] = cgist_kernel(A,At,f,mu_new,regularizer,opts);
		norm_new = norm(A(opts.guess)-f);
		multCount = multCount+count+1;
							%  If l1 energy is small compared to L2, then this
							%  is probably not a feasible problem
		feasibilityIndex = mu_new*sum(abs(opts.guess))/norm_new;
		if feasibilityIndex<1e-10
			error(['l2 constraint appears to be infeasible: mu = ' num2str(mu_new) ' norm = ' num2str(norm_new)]);
		end
	end

						%  Run this loop until we have the ideal lagrange
						%  multiplier, mu_prime
		u_prime = opts.guess;
		opts.guess = u_old;
		% prime the loop
		a = (norm_old^2-norm_new^2)/(mu_old^2 - mu_new^2);
		b = norm_old^2 - a*mu_old^2;
		mu_prime = sqrt( (eps*eps-b)/a );
	while min(abs(norm_new-eps), abs(norm_old-eps))>tol_norm
        
        %disp(['mu_new: ' num2str(mu_new) '   mu_old: ' num2str(mu_old) ...
        %    '   norm_new: ' num2str(norm_new) '   norm_old: ' num2str(norm_old)]);
		
            %  solve the optimization problem with the new multiplier
		[u_prime, count, subgradientNorm, opts.out ] = cgist_kernel(A,At,f,mu_prime,regularizer,opts);
		norm_prime = norm(A(u_prime)-f);
		multCount = multCount+count+1;
				% Pick the best two of the last 3 iterates to generate the next
			% multiplier
		if abs(norm_old-eps)<abs(norm_new-eps)
			a = (norm_old^2-norm_prime^2)/(mu_old^2 - mu_prime^2);
			b = norm_old^2 - a*mu_old^2;
		else
			a = (norm_prime^2-norm_new^2)/(mu_prime^2 - mu_new^2);
			b = norm_prime^2 - a*mu_prime^2;
		end

		if norm_prime<eps
			mu_new = mu_prime;
			norm_new = norm_prime;
		else 
			mu_old = mu_prime;
			norm_old = norm_prime;
			opts.guess = u_prime;
		end
		
		mu_next = sqrt( (eps*eps-b)/a );
		if mu_next<=mu_new || mu_next>=mu_old
            %disp('Bisection Active');
			mu_next = (mu_old+mu_new)/2;
		end
	
		mu_prime = mu_next;
	end

	sol = u_prime;
	out = opts.out;

return

function [sol, multCount, subgradientNorm, out] = cgist_l1_constrained(A,At,f,eps,regularizer,opts)

		% set the tolerance in the norm constraint
	tol_norm = opts.tol*10;
	opts.tol = opts.tol/10;	%  make kernel solution more precise so we satisfy the norm tolerance
	norm_new = 0;
	mu_new = opts.mu_star;
	multCount  = 1;
	if eps<tol_norm   %	If the zero vector is a solution, then return
		sol = opts.guess;
		subgradientNorm = 0;
		out = opts.out;
		return
	end

	while  norm_new < eps     % Do continuation until mu_new is small enough to satisfy the L2 constraint
		mu_old = mu_new;
		norm_old = norm_new;
		u_old = opts.guess;

        %  Use Wright and Nowak's adaptive continuation
        Aty = At(f-A(opts.guess))/5; 
	    mu_new = max(abs(Aty));
            
		[opts.guess, count, subgradientNorm, opts.out] = cgist_kernel(A,At,f,mu_new,regularizer,opts);
		norm_new = sum(abs(opts.guess));
		multCount = multCount+count;
							%  If l1 energy is small compared to L2, then this
							%  is probably not a feasible problem
		feasibilityIndex = mu_new*sum(abs(opts.guess))/norm_new;
		if feasibilityIndex<tol_norm
			warning('MATLAB:invalidConstraint','L1 constraint is inactive because constraint parameter is too large.'); 
			sol = opts.guess;
			out = opts.out;
			return;
		end
	end

						%  Run this loop until we have the ideal lagrange
						%  multiplier, mu_prime
	
		% prime the loop
	u_prime = opts.guess;
	opts.guess = u_old;
	lambda = (eps - norm_old)/(norm_new-norm_old);
 	mu_prime = (1-lambda)*mu_old+lambda*mu_new;
	while min(abs(norm_new-eps), abs(norm_old-eps))>tol_norm

			%  solve the optimization problem with the new multiplier
		[u_prime, count, subgradientNorm , opts.out ] = cgist_kernel(A,At,f,mu_prime,regularizer,opts);
		norm_prime = sum(abs(u_prime));
		multCount = multCount+count;
				% Pick the best two of the last 3 iterates to generate the next
			% multiplier
		if abs(norm_old-eps)<abs(norm_new-eps)
			lambda = (eps - norm_old)/(norm_prime-norm_old);
			mu_next = (1-lambda)*mu_old+lambda*mu_prime;
		else
			lambda = (eps - norm_prime)/(norm_new-norm_old);
			mu_next = (1-lambda)*mu_prime+lambda*mu_new;
		end
	
		if norm_prime>eps
			mu_new = mu_prime;
			norm_new = norm_prime;
		else 
			mu_old = mu_prime;
			norm_old = norm_prime;
			opts.guess = u_prime;
		end

		if mu_next<=mu_new || mu_next>=mu_old
			mu_next = (mu_old+mu_new)/2;
		end

		mu_prime = mu_next;
	end

	sol = u_prime;
	out = opts.out;

return

% This is the primary kernel that runs the basic cgist algorithm for basis
% pursuit
function  [u, multCount, subgradientNorm, out] = cgist_kernel(A,At,f,mu,regularizer,opts)
  %  disp(['Call to CGIST Kernel']);
	if ~isreal(mu) || mu<0
		error(['Invalid value for mu: ' num2str(mu)  ]);
	end

	if strcmp(regularizer,'l1')
		shrink = @(x,lambda) max(abs(x)-lambda,0).*sign(x);
		projectedGradient = @(g,u,mu) shrink(-g,mu).*(u == 0)-( mu*sign(u)+g ).*(u~=0);
	elseif strcmp(regularizer,'nonnegative')
			 reproject = @(x) real(x).*(real(x)>0)+1i*imag(x);
			 shrink =  @(x,lambda) max(abs(reproject(x))-lambda,0).*sign(reproject(x));  %@(x,lambda) max( real(x)-lambda, 0)+1i*max( imag(x)-lambda, 0);
			 projectedGradient = @(g,u,mu) shrink(-g,mu).*(real(u) <= 0)-( mu*sign(u)+g ).*(real(u)>0);
	else
		error(['Invalid regularizer handed to kernel: ' regularizer]);
	end


	if ~exist('projectedGradient','var')
		 error('Invalid regularizer:  Valid choices are l1, nonneg');
	end


	multCount = 0;  %  counts how many matrix-vector multiplies we have used
	out = opts.out;
	out.num_continuation_steps = out.num_continuation_steps+1;
	
	%% ---------------------------------------------
	% we have to initialize r, u, ls, g: these are values of each variable at
	% iteration zero

	u = opts.guess;
	ls = A(u)-f;  multCount = multCount+1;
	g = At(ls);  multCount = multCount+1;
	r= projectedGradient(g,u,mu);

	if opts.record_iterates
		out.iterates{end+1} = u;
	end
	if opts.record_subgradient_norms
		out.subgradient_norms(end+1) = norm(r);
	end
	if opts.record_objective
		out.objectives(end+1) = mu*sum(abs(u))+0.5*norm(ls)^2;
	end
	
	%% ---------------------------------------------
	% define line search parameters
	%C = 100*psi; % the constant makes sure that 
	%											%we are only doing the line search in extreme cases
	%Q = 0.5;
	%nu = 0.8; % the smaller nu, the higher the importance of the preceding function value
	%sigma = 0.001; % higher sigma makes it more likely that line search is used
	%rho = 0.5; % take alpha*rho as step-length if alpha fails... 
	%% ---------------------------------------------


	%% compute new alpha, and initialize the second iterate of the algorithm
	% Note that variables ending in "p1" denotes an iterate with index "i+1",
	% i.e. this is a newer value of this partiuclar iterate.
	Ar = A(r);   multCount = multCount+1;
	s = Ar'*Ar;  multCount = multCount+1;
	if sqrt(s)<10^(-12)
		 alpha = 1; % or any other value
	else
		 alpha = (r'*r)/s;
	end

	up1 = shrink( u - alpha*g, alpha*mu );
	lsp1 = A(up1)-f;  multCount = multCount+1;
	gp1 = At(lsp1);   multCount = multCount+1;

	%% main loop
	for i=2:opts.max_iter       

		%%  Store the update iterates in the original variables, i.e. iterates
		%  with index "i+1" get stored as iterates with index "i"
	rm1 = r;
	um1 = u;
	u = up1;
	lsm1 = ls;
	ls = lsp1;
	gm1=g;
	g = gp1;
	psi = 0.5*norm(ls)^2 + mu*sum(abs(u));

		  %% compute the new gradient/residual
	r = projectedGradient(g,u,mu);
	
	if opts.record_iterates
		out.iterates{end+1} = u;
	end
	if opts.record_subgradient_norms
		out.subgradient_norms(end+1) = norm(r);
	end
	if opts.record_objective
		out.objectives(end+1) = mu*sum(abs(u))+0.5*norm(ls)^2;
	end
	
		   %%  check weather the gradient is sufficiently small to terminate the
		   %  algorithm.  Use 2_norm_of_residual/(support_size+1) for comparison.
	subgradientNorm = norm(r)/sum(size(find(u~=0)));
	if( subgradientNorm < opts.tol)
		out.mults_per_continuation_step(end+1) = multCount;
		out.iterates_per_continuation_step(end+1) = i;
		return;
	end

	%% compute new alpha (the step size)
	Ar = A(r);  multCount = multCount+1;
	s = Ar'*Ar;  multCount = multCount+1;
	if s<10^(-12)
		alpha = 1; % or any other value
	else
		alpha = (r'*r)/s;
	end

	
	%%  Do a forward backward splitting step to get the next iterate
	up1 = shrink( u- alpha*g, alpha*mu );
	uh = up1;  % this value is used for non-monotone line search, but inactive for monotone line search
	
	%disp(['alpha = ' num2str(alpha)]);
	
	%%  Check weather the active set changed
	if(norm(sign(up1)-sign(u)) + norm(sign(um1)-sign(u)) < .5)
		%if the active set remains constant, then do a conjugate gradient step
		lshat = ls + alpha*Ar;
		if norm(imag(up1))>0 || norm(imag(u))>0 
			lshat = A(uh)-f; multCount = multCount+1;
		end
		ghat = At(lshat);  multCount = multCount+1;
		rhat = r-(ghat-g).*(u~=0);
		beta = real(rm1'*rhat)/(rm1'*rm1);
		
		% top is the furthers we can move in the CG direction without changing
		% the active set
		if norm(imag(up1))==0 && norm(imag(um1))==0 
			top = up1./(um1+(um1==0));
			top = min(top+1e10*(um1==0));
			if(top<=beta)
				 beta=top;
			end
		end
		
		%  Make sure beta lies within the valid range
		beta = min(max(beta,0),1-1e-3);
		
		%  set up1 (the next iterate) to the CG updated iterate
		up1 = (uh-beta*um1)/(1-beta);
		lsp1 = (lshat-beta*lsm1)/(1-beta);  
		gp1 = (ghat-beta*gm1)/(1-beta);  

	else
	   %  If the active set is not constant, then don't accelerate, and simply
	   %  accept the result for forward backward splitting
	lsp1 = A(up1)-f;  multCount = multCount+1;
	gp1 = At(lsp1);    multCount = multCount+1;

	end

	l1_up1 = sum(abs(up1));
	
	%-------------------------------------------------
	% NMLS in FPC_AS
	%{
	d = up1 - u;
	delta = g'*d + mu*( l1_up1 - l1_u );
	psi = 1/2*norm(lsp1)^2 + mu*l1_up1;
	alpha = 1;
	if psi > C + sigma*alpha*delta
		 up1 = uh;
		 lsp1 = A(up1)-f; multCount=multCount+1;
		 gp1 = At(lsp1);  multCount=multCount+1;
		 l1_up1 = sum(abs(up1));
		 d = up1 - u;
		 delta = g'*d + mu*( l1_up1 - l1_u );
		 psi = 1/2*norm(lsp1)^2 + mu*l1_up1;
	while psi > C + sigma*alpha*delta
		% disp('LS')
		 alpha = alpha*rho;
		 up1 = u + alpha*d;
		 ls = A(up1)-f; multCount = multCount+1;
		 gp1 = At(ls); multCount = multCount+1;
		 q = shrink(gp1,mu);
		 r = -1*( mu*sign(up1)+gp1 ).*(up1~=0) - q.*(up1==0);
		 if norm(r) < 10^(-8);
			 break % minimizer found
		 end
		 l1_up1 = sum(abs(up1));
		 psi = 1/2*norm(ls)^2 + mu*l1_up1;
	end
	end
	Q = nu*Q+1; % converges to 1 -> new value gets more and more important
	C = ((Q-1)*C+psi)/Q;
	%}

	%-------------------------------------------------
	%%{

	% monotone line searchs
	% begin line  search along u - alpha*g (for that we needed g and ls1)
	% line search 2
	
	psip1 = 1/2*norm(lsp1)^2 + mu*l1_up1;
	if psip1 - psi > 1e-9
		alpha=min(alpha*0.1,10);
		psi_new = psi+1;
		%disp(['LINE SEARCH psi = ' num2str(psi) '  psip1 = ' num2str(psip1) '  diff = ' num2str(psi-psip1)]);
	count=0;
	while psi_new>psi*(1-1e-9) && count<5
		up1 = shrink( u- alpha*g, alpha*mu );
		lsp1 = A(up1)-f; multCount = multCount+1;
		psi_new = 0.5*norm(lsp1)^2+mu*sum(abs(up1));
		
		count = count+1;

 		%disp(['norm = ' num2str(norm(At(A(u)-f)-g)) '   alpha = ' num2str(alpha) '  E = ' num2str( 0.5*norm(A(u)-f).^2+50*sum(abs(u)))]);

		alpha = alpha/100;
	end
	gp1 = At(lsp1);    multCount = multCount+1;
	%disp(['                         COUNT = ' num2str(count)  '   e = ' num2str(psi_new) ]);
	if(count == 5 || psi-psi_new<psi*1e-10)
		return;
	end
	
	%}
	% end line search
	%------------------------------------------------- 

	end
	
	end
return

function opts = completeOpts(N,opts)
	if ~isfield(opts,'guess')
		opts.guess = zeros(N,1);
	elseif size(opts.guess,1) ~= N || size(opts.guess,2)~=1
		error('Initial guess for cgist has improper dimensions')
	end
	if ~isfield(opts,'continuation_if_needed')
		opts.continuation_if_needed = true;
	end
	if ~isfield(opts,'tol')
		opts.tol = 1e-4;
	end
	if ~isfield(opts,'max_iter')
		opts.max_iter = 25e3;
	end
	if ~isfield(opts,'record_objective')
		opts.record_objective = false;
	end
	if ~isfield(opts,'record_iterates')
		opts.record_iterates = false;
	end
	if ~isfield(opts,'record_subgradient_norms')
		opts.record_subgradient_norms = false;
	end
	if ~isfield(opts,'record_mults_per_continuation_step')
		opts.record_mults_per_continuation_step = false;
	end
	
	opts.out = [];
	opts.out.num_continuation_steps = 0;
	opts.out.iterates = {};
	opts.out.subgradient_norms = [];
	opts.out.mults_per_continuation_step=[];
	opts.out.iterates_per_continuation_step=[];
	opts.out.objectives = [];

	
return
